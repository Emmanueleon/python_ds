{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solucionar problemas con archivos csv\n",
    "---\n",
    "\n",
    "CSV significa **valores separados por comas**. Sin embargo, un archivo CSV no tiene que usar solo una coma como **delimitador**; se puede usar cualquier carácter. \n",
    "\n",
    "A veces pueden aparecer como archivos `.tsv` o `.tab` (también conocidos como archivos TSV) además de `.csv`.\n",
    "\n",
    "Existen formas de lidiar con estos problemas: \n",
    "1. Usar el argumento `sep`. \n",
    "2. Indicar nombre de encabezados con `header = None` y `name=`. \n",
    "3. Renombrar encabezados con `header = None`y `rename()`. \n",
    "4. Indicar tipo de decimales con el argumento `decimal = `. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/datasets/gpp_modified.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Uso de tipo de separador, por defecto \",\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/datasets/gpp_modified.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/gpp_modified.csv'"
     ]
    }
   ],
   "source": [
    "## Uso de tipo de separador, por defecto \",\"\n",
    "data = pd.read_csv('/datasets/gpp_modified.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indicar nombre de encabezados \n",
    "column_names = [\n",
    "    'country',\n",
    "    'name',\n",
    "    'capacity_mw',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'primary_fuel',\n",
    "    'owner'\n",
    "    ]\n",
    "\n",
    "data = pd.read_csv('/datasets/gpp_modified.csv', header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renombrar encabezado\n",
    "data = pd.read_csv('/datasets/gpp_modified.csv', header=None)\n",
    "\n",
    "data = data.rename(columns = {0: \"country\",1: \"name\", 2:\"capacity_mw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indicar el tipo de decimal \n",
    "data = pd.read_csv('/datasets/gpp_modified.csv', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer archivos excel\n",
    "---\n",
    "\n",
    "Pandas proporciona la función `read_excel()` para leer archivos Excel \n",
    "\n",
    "Por defecto, esta función carga la primera hoja, pero un archivo Excel puede contener varias hojas. Para tal caso utilizar el parámetro `sheet_name=` y especificar el nombre o el número de la hoja que queremos seleccionar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Abrir archivo excel\n",
    "### Con nombre de la hoja de cálculo \n",
    "df = pd.read_excel('/datasets/product_reviews.xlsx', sheet_name='reviewers')\n",
    "\n",
    "### Con número de la hoja de cálculo \n",
    "df = pd.read_excel('/datasets/product_reviews.xlsx', sheet_name=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspección de los datos\n",
    "---\n",
    "\n",
    "Echar un vistazo a tus datos es útil cuando empiezas a trabajar con un nuevo dataset porque te ayudan a plantear las primeras preguntas que debes explorar. Algunos de los atributos y métodos incluyen: \n",
    "\n",
    "- `info()`. Imprime información general sobre el DataFrame\n",
    "- `shape()`. Devuelve tanto el número de filas como el número de columnas en el dataset. \n",
    "- `sample()`. Selecciona filas aleatorias del DataFrame en lugar de filas consecutivas del principio o del final del DataFrame. \n",
    "- `describe()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estructura del dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la siguiente información: \n",
    "\n",
    "- El número de filas (RangeIndex: __ entries);\n",
    "- El número de columnas (total __ columns);\n",
    "- El nombre de cada columna (Column);\n",
    "- El número de valores de cada columna que no están ausentes (Non-Null Count);\n",
    "- El tipo de datos de cada columna (Dtype)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Almacenar número de filas y columnas como variables\n",
    "n_rows, n_cols = df.shape\n",
    "\n",
    "print(f\" El dataframe tiene {n_rows} filas y {n_cols} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `shape` devuelve una **tupla** como salida. \n",
    "\n",
    "Una tupla es un tipo de datos similar a una lista de Python en términos de indexación, objetos anidados y repetición. Sin embargo, la principal diferencia entre ambas es que una tupla Python es inmutable (no puede modificarse), mientras que una lista Python es mutable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder tener una mejor visión del dataframe, podemos combinar el método `info()` y otro métodos como `head()` o `tail()`. Sin embargo,para poder observar una mejor muestra de los datos que se encuentran en el dataframe y no solo los encabezados y la última parte se puede usar el método `sample()`. Si quiero que haya repetibilidad en mi aleatoriedad agregar el argumento `random_satet()` y establecer y algún valor entero de tu elección (cualquier número entero entre 0 y 4294967295)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encabezados\n",
    "print(data.head(10))\n",
    "\n",
    "## Parte final \n",
    "print(data.tail(10))\n",
    "\n",
    "## Aleatorio\n",
    "print(data.sample(10))\n",
    "\n",
    "## Aleatoriedad establecida\n",
    "print(data.sample(10, random_state= 1989))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `describe()` es muy útil para obtener información sobre las columnas numéricas de tus datos. La salida incluye estadísticas de resumen.   \n",
    "\n",
    "Es aconsejable que además, el análisis se acompañe de visualizaciones de datos para obtener una imagen completa, , ya que es posible que sus estructuras sean muy diferentes aunque tengan estadísticas resumidas similares (como el cuarteto de Anscombe). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Método describe()\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera predeterminada, se ignoran las columnas no numéricas. Para poder incluir otro tipo de columnas no numéricas se utiliza el parámetro `include =` con el tipo de datos que queremos añadir p.e, `object` u añadir todas las columnas `all`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajar con valores ausentes y duplicados\n",
    "---\n",
    "\n",
    "### Contar valores ausentes\n",
    "Una buena manera de empezar a comprobar los valores ausentes es llamar al método `info()` de tu DataFrame. Los valores nulos son valores ausentes, mientras que los no nulos son valores no ausentes. \n",
    "\n",
    "Una vez identificado el número de observaciones podemos determinar el número de valores ausentes con `isna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determinar la información del dataframe\n",
    "data.info()\n",
    "\n",
    "## Contar el número de valores ausentes de cada columna\n",
    "data.isna().sum()\n",
    "\n",
    "## Contar el número de valores ausentes totales\n",
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es con el método `value_counts()`, que devuelve la cantidad de veces que cada valor único aparece en esa columna. Este método es conveniente utilizarlo sobre una solo columna o *series*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conocer el número de valores únicos para la columna source\n",
    "print(df_logs['source'].value_counts(dropna=False)) # drop_na=False permite contar el número de Nas en la columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida se ordena en orden descendente según el recuento de cada valor. Alternativamente, podemos ordenar la salida alfabéticamente según los nombres de los valores. Para hacerlo, podemos utilizar el método `sort_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ordenar el resultado de acuerdo con el index y no de acuerdo con los valores de la columna\n",
    "print(df_logs['source'].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar Dataframes con NaNs\n",
    "\n",
    "Para examinar las filas auseentes del dataframe, una de las maneras es utilizar el método `is.na()`.  El resultado genera una serie con los valores ausente `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtra los valores con NaNs\n",
    "print(df_logs[df_logs['source'].isna()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sin embargo, a veces no es eso lo que nos convien. Para ello resulta más útil combinar `~ `con `isna()` para filtrar las filas con valores ausentes. La adición del símbolo de tilde (~), invierte el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtra los valores sin NaNs\n",
    "print(df_logs[~df_logs['source'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible filtrar un dataframe a partir de **múltiples condiciones de filtrado**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar el df donde no haya valores ausentes en la columna \"email\" \n",
    "# y que solo sean valores de email de la columna source\n",
    "print(df_logs[(~df_logs['email'].isna()) & (df_logs['source'] == 'email')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de filtrado anterior consta de dos partes:\n",
    "\n",
    "1. `(~df_logs['email'].isna())` devuelve una serie de booleanos donde `True` indica que no falta ningún valor en la columna `'email'`.\n",
    "\n",
    "2. `(df_logs['source'] == 'email')` devuelve una serie de booleanos, donde `True` indica que `'source'` tiene `'email'` como valor, y `False` indica lo contrario.\n",
    "\n",
    "3. Comprobamos dos series de booleanos para ver dónde ambas condiciones devuelven `True`. Utilizamos el símbolo `&` para representar el operador lógico `and`. Las filas que cumplen ambas condiciones (es decir, que cumplen la primera condición y la segunda) se incluyen en el resultado final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rellenar los valores categóricos ausentes\n",
    "\n",
    "Como recordatorio, las **variables categóricas** o **cualitativas** representan un conjunto de valores posibles que puede tener una observación particular. Es posible que tengan un orden en particular, por lo que serían **ordinales** o pueden no tener un orden en particular, por lo que serían **nominales**. \n",
    "\n",
    "Podemos sustituir los valores ausentes de las columnas con **valores por defecto** por ejemplo, una cadena vacía `''` . Esto lo podemos realizar con el método `fillna()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sustituir valores ausentes\n",
    "df_logs['email'] = df_logs[\"email\"].fillna(value= \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar `fillna()` no es la única forma en que podemos rellenar los valores ausentes con cadenas vacías. También podemos hacerlo directamente al leer los datos mediante `read_csv()` utilizando el parámetro `keep_default_na = False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargar el dataset haciendo que en vez de que sean NaN = TRUE sea FALSE\n",
    "df_logs = pd.read_csv('/datasets/visit_log.csv', keep_default_na=False)\n",
    "\n",
    "print(df_logs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Nota:</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Ten en cuenta que establecer `keep_default_na=False` convierte todos los valores ausentes **en cadenas vacías**, incluso para columnas numéricas. Esto hace que las columnas numéricas se lean como cadenas cuando tienen valores ausentes.   \n",
    "\n",
    "Así que asegúrate de usar solo `keep_default_na=False` cuando desees que todos los valores ausentes en cada columna se lean como cadenas vacías.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que, querramos sustituir los valores de defecto por algún otro valor. En tal caso es útil emplear el método `replace()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remplazar el valor de defecto \"\" por otro valor\n",
    "df_logs['source'] = df_logs[\"source\"].replace(\"\", \"email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rellenar los valores ausentes cualitativos \n",
    "\n",
    "Como recordatorio, las variables **cuantitativas** tienen valores numéricos que podemos usar para cálculos aritméticos, por ejemplo, la altura, el peso, la edad y los ingresos. En Python, estos valores tienden a almacenarse como números enteros o flotantes.\n",
    "\n",
    "Debido a que queremos hacer cálculos numéricos con estas columnas, no podemos rellenar esos valores con cadenas como `'Unknown'` o `''`. En su lugar, debemos rellenarlos con valores representativos apropiados. Para estos valores se suele utilizar la **media** o la **mediana** del conjunto de datos.\n",
    "\n",
    "La elección entre media o mediana dependerá de la uniformidad de los valores, es decir, de su **distribución**. \n",
    "\n",
    "\n",
    "Para rellenar los valores ausentes, podemos seguir estos pasos:\n",
    "\n",
    "1. Determina la distribución de los datos.\n",
    "\n",
    "2. Si no hay valores atípicos significativos, calcula la media utilizando el método `mean()`.\n",
    "\n",
    "3. Si tus datos tienen valores atípicos significativos, calcula la mediana utilizando el método `median()`.\n",
    "\n",
    "4. Reemplaza los valores ausentes con la media o la mediana utilizando el método `fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remplazar el promedio \n",
    "### Determinar el promedio \n",
    "age_avg = analytics_data['age'].mean()\n",
    "print(\"Mean age:\", age_avg)\n",
    "\n",
    "### Remplazar la columna con los valores promedio\n",
    "analytics_data['age'] = analytics_data['age'].fillna(age_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Nota:</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "También vale la pena señalar que a veces no necesitamos rellenar los valores ausentes en absoluto.  Por ejemplo, si solo falta una pequeña parte de tus datos, y los datos ausentes son aleatorios, podría ser buena idea dejar los valores como NaN, en cuyo caso simplemente no se incluirían en ningún cálculo numérico.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestión de duplicados\n",
    "\n",
    "Otro de los problemas comunes en la etapa de procesamiento de datos en las bases de datos son los valores duplicados. \n",
    "\n",
    "Hay dos técnicas que funcionan para encontrar datos duplicados:\n",
    "\n",
    "1. Podemos utilizar el método `duplicated()` junto con `sum()` para obtener el número de valores duplicados en una sola columna o filas duplicadas en un DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columna booleana con False si no es duplicada y True si lo es \n",
    "print(df.duplicated())\n",
    "\n",
    "## Conteo de las filas duplicadas\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "## Conocer las filas duplicadas\n",
    "print(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Podemos utilizar el método `value_counts()`. Este método identifica todos los valores unívocos en una columna y calcula cuántas veces aparece cada uno. Podemos aplicar este método a los Series para obtener los pares valor-frecuencia en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conocer el número de veces que determinada fila es similar\n",
    "print(df['col_1'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante notar que mediante este método solo se inspecciona la columna seleccionada, puede que no sea un **duplicado explicito** y solo se repita el valor de dicha columna y no de las subsecuentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de que existan **filas completamente duplicadas**, se pueden tratar utilizando el método `drop_duplicates()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eliminar filas duplicadas explícitas\n",
    "print(df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si solo deseas considerar duplicados en una (o algunas) de las columnas en lugar de filas completamente duplicadas, puedes usar el parámetro `subset=`. Pásale el nombre de la columna (o la lista de nombres de columna) donde deseas buscar duplicados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eliminar filas seleccionadas \n",
    "print(df.drop_duplicates(subset='col_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto eliminará las columnas con valores similares, pero puede eliminar otras columnas donde los valores sean diferentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que después de eliminar los duplicados, tenemos que llamar al método `reset_index()` con el parámetro `drop=True`. Esto nos permite arreglar la indexación y eliminar el índice antiguo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen casos en donde existan **duplicados implícitos** y puedan existir *typos*. Para ello, la forma más sencilla de manejar entradas duplicadas como estas es homogeneizar los términos de acuerdo con las buenas prácticas de programación. \n",
    "\n",
    "Por ejemplo, hacer que todas las letras de cadenas estén en minúsculas, utilizando el método `lower()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convertie los strings en mínusculas\n",
    "print(df['col_1'].str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de `df['col_1']`, tenemos `.str`, que nos permite aplicar métodos de cadena directamente a la columna. Esto es necesario para poder aplicar el método `lower()` en el paso siguiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que el cambio se preserve, ten en cuenta que se debe **sustituir** la columna original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sustituir la columna col_1\n",
    "df['col_1'] = df['col_1'].str.lower()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción para cambiar duplicados, sin necesidad de cambiar mayúsculas por minúsculas, es cambiar todas las apariciones específicas de una columnas por otro string, utilizando el método `replace()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remplazar un string por otro\n",
    "df['category_modified'] = df['category'].str.replace('tbc', 'baby care')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no estás seguro de cuál es el mejor enfoque, siempre puedes conservar la columna original y crear una nueva columna adicional, con las cadenas modificadas. Por ejemplo, podrías guardar el resultado de la sustitución en la columna `'item'` en una nueva columna llamada `'item_modified'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creación de nueva columa \n",
    "df['category_modified'] = df['category'].str.replace('tbc', 'baby care')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente otra aproximación para poder cambiar un valor específico es utilizar el método `loc[]`. Para ello: \n",
    "\n",
    "1. Buscamos la fila que contiene el valor que queremos sustituir. \n",
    "2. Pasamos el índice y el nombre de columna adecuados a `loc[]`, y utiliza el signo `=` para establecer el valor deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en conjunto\n",
    "\n",
    "## Determinar el promedio\n",
    "avg_per_category = df.groupby('category')['price'].mean()\n",
    "\n",
    "## Extraer del dataframe\n",
    "mean_val = avg_per_category[2]\n",
    "\n",
    "## Sustituir mediante loc[]\n",
    "df.loc[7,\"price\"] = mean_val# escribe tu código aquí\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de datos\n",
    "---\n",
    "\n",
    "### Atributo index\n",
    "Los objetos Series y DataFrame en pandas siempre tienen índices que se almacenan en el atributo index. Cada vez que creas un Series o un DataFrame, su atributo de index se crea automáticamente con valores por defecto si no especificas los valores del índice.\n",
    "\n",
    "Existen dos maneras de establecer valores de índice:\n",
    "\n",
    "- Pasar los valores del índice al parámetro `index=` al crear un DataFrame o un Series.\n",
    "- Asignar los valores del índice al atributo `index` de un DataFrame o Series existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A     Pacific\n",
      "B    Atlantic\n",
      "C      Indian\n",
      "D    Southern\n",
      "E      Arctic\n",
      "dtype: object\n",
      "\n",
      "1     Pacific\n",
      "2    Atlantic\n",
      "3      Indian\n",
      "4    Southern\n",
      "5      Arctic\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Manera 1: Pasar los valores al parámetro index\n",
    "oceans = pd.Series(['Pacific', 'Atlantic', 'Indian', 'Southern', 'Arctic'],\n",
    "                   index=['A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "print(oceans)\n",
    "print()\n",
    "\n",
    "## Manera 2: Asignar los valores al atributo index\n",
    "oceans = pd.Series(['Pacific', 'Atlantic', 'Indian', 'Southern', 'Arctic'])\n",
    "\n",
    "oceans.index = [1, 2, 3, 4, 5]\n",
    "print(oceans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de los DataFrames, existe otra forma de establecer los valores del índice mediante el método `set_index()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          flower                          insect\n",
      "state                                                           \n",
      "Alabama                 Camellia               Monarch butterfly\n",
      "Alaska             Forget-me-not  Four-spotted skimmer dragonfly\n",
      "Arizona   Saguaro cactus blossom          Two-tailed swallowtail\n",
      "Arkansas           Apple blossom              European honey bee\n",
      "\n",
      "Index(['Alabama', 'Alaska', 'Arizona', 'Arkansas'], dtype='object', name='state')\n"
     ]
    }
   ],
   "source": [
    "states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']\n",
    "flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']\n",
    "insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']\n",
    "index   = ['state 1', 'state 2', 'state 3', 'state 4']\n",
    "\n",
    "df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)\n",
    "df = df.set_index('state') # reemplazar el índice\n",
    "\n",
    "print(df)\n",
    "print()\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originalmente, nuestros índices eran números de estado: 'state 1', 'state 2' etc. Después, los sustituimos por los valores de la columna 'state'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no quieres que el índice tenga un nombre, puedes eliminarlo estableciendo el atributo index_name de un DataFrame a None. Así es como puedes hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          flower                          insect\n",
      "Alabama                 Camellia               Monarch butterfly\n",
      "Alaska             Forget-me-not  Four-spotted skimmer dragonfly\n",
      "Arizona   Saguaro cactus blossom          Two-tailed swallowtail\n",
      "Arkansas           Apple blossom              European honey bee\n",
      "\n",
      "Index(['Alabama', 'Alaska', 'Arizona', 'Arkansas'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']\n",
    "flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']\n",
    "insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']\n",
    "index   = ['state 1', 'state 2', 'state 3', 'state 4']\n",
    "\n",
    "df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)\n",
    "df = df.set_index('state')\n",
    "\n",
    "df.index.name = None\n",
    "print(df)\n",
    "print()\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexación \n",
    "Ahora vamos a hablar de **indexación**. La terminología puede ser confusa, así que ten en cuenta estas definiciones:\n",
    "\n",
    "- Index (índice): un componente de un Series o DataFrame, accesible mediante el atributo `index`.\n",
    "- Indexing (indexación): el proceso de acceder a los valores de un Series o DataFrame utilizando sus índices.\n",
    "  \n",
    "Es posible utilizar el atributo `loc[]` para acceder a los elementos del DataFrame utilizando los valores de los índices y los nombres de las columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         flower                  insect\n",
      "state 1                Camellia       Monarch butterfly\n",
      "state 3  Saguaro cactus blossom  Two-tailed swallowtail\n"
     ]
    }
   ],
   "source": [
    "# Filtrado mediante uso de índices\n",
    "## Creación listas para dataframe\n",
    "states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']\n",
    "flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']\n",
    "insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']\n",
    "index   = ['state 1', 'state 2', 'state 3', 'state 4']\n",
    "\n",
    "df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)\n",
    "\n",
    "## Filtrado\n",
    "filtered_df = df.loc[['state 1', 'state 3'], ['flower', 'insect']]\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener un rango de índices para una **única columna o fila**, solo hay que especificar el primer y el último índice separados por un `:`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 1                  Camellia\n",
      "state 2             Forget-me-not\n",
      "state 3    Saguaro cactus blossom\n",
      "Name: flower, dtype: object\n",
      "\n",
      "flower             Camellia\n",
      "insect    Monarch butterfly\n",
      "Name: state 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Indexar una sola solumna\n",
    "print(df.loc['state 1': 'state 3', 'flower'])\n",
    "print() \n",
    "\n",
    "## Indexar una sola fila\n",
    "print(df.loc[\"state 1\", \"flower\":\"insect\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, puedes seleccionar **múltiples columnas así como índices**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         flower                          insect\n",
      "state 1                Camellia               Monarch butterfly\n",
      "state 2           Forget-me-not  Four-spotted skimmer dragonfly\n",
      "state 3  Saguaro cactus blossom          Two-tailed swallowtail\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[\"state 1\": \"state 3\", \"flower\": \"insect\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar el método `iloc[]` es muy similar. La principal diferencia entre métodos, es que `loc[]` utiliza el índice y las _etiquetas_ de columnas para acceder a los elementos, mientras que `iloc[]` utiliza _enteros_ para designar las posiciones de los elementos que necesitas obtener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "European honey bee\n"
     ]
    }
   ],
   "source": [
    "## Uso de iloc para seleccionar el elemento de la fila 4 y la columna 3\n",
    "print(df.iloc[3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera que con `loc[]`, podemos acceder a múltiples filas y/o columnas con `iloc[]` pasándole listas de sus posiciones o utilizando el slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                flower                          insect\n",
      "state 2  Forget-me-not  Four-spotted skimmer dragonfly\n",
      "state 4  Apple blossom              European honey bee\n",
      "\n",
      "                flower                          insect\n",
      "state 1       Camellia               Monarch butterfly\n",
      "state 2  Forget-me-not  Four-spotted skimmer dragonfly\n"
     ]
    }
   ],
   "source": [
    "## Indexación utilizando celdas específicas\n",
    "print(df.iloc[[1,3], [1,2]])\n",
    "print()\n",
    "\n",
    "## Indexación utilizando rangos\n",
    "print(df.iloc[0:2, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar al utilizar rangos no es necesario poner los índices entre corchetes `[]`. En cambio si vamos a definir celdas específicas se necesita definirlas con los corchetes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usar `iloc[]` puede ser más conveniente si sabes que solo quieres ver las primeras filas o columnas, o cuando sabes el número exacto de fila o columna que necesitas. `iloc[]` también puede ser útil como un atajo para ahorrar tiempo al escribir nombres de columnas o etiquetas de índices.\n",
    "\n",
    "- Usar `loc[]` puede lograr una mejor legibilidad y comprensión de lo que tu código está haciendo, tanto para tus colegas que leen tu código como para tu yo del futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado personalizado mediante query()\n",
    "\n",
    "Para filtrar DataFrames utilizando operadores lógicos para crear una serie de Booleanos que llamaremos **máscara booleana** a partir de ahora. Esta máscara nos permitirá accesar a la condición que queremos al implementarla sobre el df original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Establesco la condición y creo el df booleano (máscara)\n",
    "mask = df['jp_sales'] >= 1\n",
    "\n",
    "## Pongo al df la máscara booleana y selecciona aquellas columnas que em interesan\n",
    "print(df[mask][['name', 'jp_sales']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos realizar este mismo filtrado utilizando el método `query()`. Todo lo que se tiene que hacer es uestablecer la condición que queríamos filtrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LA misma condición establecida previamente\n",
    "print(df.query(\"jp_sales > 1\")[['name', 'jp_sales']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para filtrar con `query()` basándose en comparaciones de cadenas, es necesario poner comillas alrededor de la cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado mediante método isin()\n",
    "\n",
    "En lugar de utilizar los operadores lógicos conocidos, `isin()` comprueba si los valores de una columna coinciden con alguno de los valores de otra matriz, como una lista o un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lista de interés \n",
    "handhelds = ['3DS', 'DS', 'GB', 'GBA', 'PSP']\n",
    "\n",
    "## Filtrado de la lista d einterés\n",
    "print(df[df['platform'].isin(handhelds)][['name', 'platform']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos utilizar la tilde (`~`) para extraer sólo las filas en las que los valores de la columna `'platform'` **no estén** en la lista `handhelds`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lista de interés \n",
    "handhelds = ['3DS', 'DS', 'GB', 'GBA', 'PSP']\n",
    "\n",
    "## Exclusión de valores en la lista de interés\n",
    "print(df[~df['platform'].isin(handhelds)][['name', 'platform']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos comprobar la presencia utilizando el método query() con la palabra clave in en nuestra cadena de consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lista de interés\n",
    "handhelds = ['3DS', 'DS', 'GB', 'GBA', 'PSP']\n",
    "\n",
    "print(df.query(\"platform in @handhelds\")[['name', 'platform']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, puedes invertirlo con la palabra clave `not in`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lista de exclusión \n",
    "handhelds = ['3DS', 'DS', 'GB', 'GBA', 'PSP']\n",
    "\n",
    "## Exclusión mediante método query\n",
    "print(df.query(\"platform not in @handhelds\")[['name', 'platform']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de estructuras de datos para el filtro de dataframes\n",
    "\n",
    "Además de utilizar índices y cadenas para filtrar datos con `query()`, también es posible filtrar utilizando **estructuras de datos** como diccionarios, Series e incluso otros DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Con un diccionario**. Para comprobar la presencia de valores de la columna 'a' entre los valores del diccionario, tenemos que utilizar el método `values()` del diccionario en nuestra consulta `\"a in @our_dict.values()\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a  b  c\n",
      "0   2  5  X\n",
      "1   3  4  Y\n",
      "2  10  3  Y\n",
      "3  11  2  Y\n",
      "4  12  1  Z\n",
      "\n",
      "{0: 10, 3: 11, 12: 12}\n",
      "\n",
      "    a  b  c\n",
      "2  10  3  Y\n",
      "3  11  2  Y\n",
      "4  12  1  Z\n"
     ]
    }
   ],
   "source": [
    "# Filtro con diccionario \n",
    "## Creación del diccionario \n",
    "our_dict = {0: 10, 3: 11, 12: 12}\n",
    "\n",
    "## Creación del df\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'a': [2, 3, 10, 11, 12],\n",
    "        'b': [5, 4, 3, 2, 1],\n",
    "        'c': ['X', 'Y', 'Y', 'Y', 'Z'],\n",
    "    }\n",
    ")\n",
    "print(df)\n",
    "print()\n",
    "print(our_dict)\n",
    "print()\n",
    "\n",
    "## Filtrado de los valores, por defecto es `.keys()`\n",
    "print(df.query(\"a in @our_dict.values()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar las claves y no los valores, necesitamos utilizar el query `a in @our_dict.keys()` o solo `\"a in @our_dict\"` ya que se comprueban las claves por defecto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a  b  c\n",
      "1   3  4  Y\n",
      "4  12  1  Z\n"
     ]
    }
   ],
   "source": [
    "## Comprobar claves en lugar de valores\n",
    "print(df.query(\"a in @our_dict\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Con un series**. Tal y como los diccionarios almacenan pares clave-valor, los objetos Series almacenan pares índice-valor. Sin embargo, en el caso de los objetos Series, los valores se comprueban por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a  b  c\n",
      "0   2  5  X\n",
      "1   3  4  Y\n",
      "2  10  3  Y\n",
      "3  11  2  Y\n",
      "4  12  1  Z\n",
      "\n",
      "X    10\n",
      "Y    11\n",
      "T    12\n",
      "dtype: int64\n",
      "\n",
      "    a  b  c\n",
      "2  10  3  Y\n",
      "3  11  2  Y\n",
      "4  12  1  Z\n"
     ]
    }
   ],
   "source": [
    "# Filtro con series \n",
    "## Creación de la serie \n",
    "our_series = pd.Series([10, 11, 12], index=['X', 'Y', 'T'])\n",
    "\n",
    "## Creación del dataframe\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'a': [2, 3, 10, 11, 12],\n",
    "        'b': [5, 4, 3, 2, 1],\n",
    "        'c': ['X', 'Y', 'Y', 'Y', 'Z'],\n",
    "    }\n",
    ")\n",
    "print(df)\n",
    "print()\n",
    "print(our_series)\n",
    "print()\n",
    "\n",
    "## Filtrado de la series\n",
    "print(df.query(\"a in @our_series\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar si en vez de los valores son los índices utilizamos query agregando los índices `a in \"our_series.index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a  b  c\n",
      "0   2  5  X\n",
      "1   3  4  Y\n",
      "2  10  3  Y\n",
      "3  11  2  Y\n"
     ]
    }
   ],
   "source": [
    "## Filtrado por índice y no por valor\n",
    "print(df.query(\"c in @our_series.index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Con un dataframe**. PTambién podemos utilizar un DataFrame externo para filtrar nuestros datos de dos maneras:    \n",
    "  \n",
    "   1. Filtrado utilizando sus valores índice\n",
    "   2. Filtrado utilizando valores de columnas específicas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el pirmer caso, basándonos en la inclusión entre los valores del índice de un DataFrame externo, simplemente lo tenemos que comprobar de la misma manera que lo hicimos para un índice de Series: accedemos al atributo `index` en nuestra consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtado utilizando otra dataframe\n",
    "## Creación df a filtrar (df1)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'a': [2, 3, 10, 11, 12],\n",
    "        'b': [5, 4, 3, 2, 1],\n",
    "        'c': ['X', 'Y', 'Y', 'Y', 'Z'],\n",
    "    }\n",
    ")\n",
    "\n",
    "## Creación dataframe que usaremos para filtrar (df2)\n",
    "our_df = pd.DataFrame(\n",
    "    {\n",
    "        'a1': [2, 4, 6],\n",
    "        'b1': [3, 2, 2],\n",
    "        'c1': ['A', 'B', 'C'],\n",
    "    },\n",
    "    index=['Z', 'X', 'P']\n",
    ")\n",
    "\n",
    "print(df)\n",
    "print()\n",
    "print(our_df)\n",
    "print()\n",
    "\n",
    "## Filtro df1 de acuerdo con df2\n",
    "print(df.query(\"c in @our_df.index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar si los valores de la columna del DataFrame que queremos filtrar (df en este caso) también están presentes en la columna de un DataFrame externo (our_df), tenemos que especificar la columna externa en nuestra consulta utilizando la **notación de puntos**.\n",
    "\n",
    "La consulta `\"a in @our_df.b1\"` comprueba si algunos valores de la columna `'a'` de df están presentes en la columna `'b1'` de `our_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a  b  c\n",
      "0   2  5  X\n",
      "1   3  4  Y\n",
      "2  10  3  Y\n",
      "3  11  2  Y\n",
      "4  12  1  Z\n",
      "\n",
      "   a1  b1 c1\n",
      "Z   2   3  A\n",
      "X   4   2  B\n",
      "P   6   2  C\n",
      "\n",
      "   a  b  c\n",
      "0  2  5  X\n",
      "1  3  4  Y\n"
     ]
    }
   ],
   "source": [
    "# Filtrado de df con notación de puntos\n",
    "## Creación df a filtrar (df1)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'a': [2, 3, 10, 11, 12],\n",
    "        'b': [5, 4, 3, 2, 1],\n",
    "        'c': ['X', 'Y', 'Y', 'Y', 'Z'],\n",
    "    }\n",
    ")\n",
    "\n",
    "## Creación dataframe que usaremos para filtrar (df2)\n",
    "our_df = pd.DataFrame(\n",
    "    {\n",
    "        'a1': [2, 4, 6],\n",
    "        'b1': [3, 2, 2],\n",
    "        'c1': ['A', 'B', 'C'],\n",
    "    },\n",
    "    index=['Z', 'X', 'P']\n",
    ")\n",
    "print(df)\n",
    "print() \n",
    "print(our_df)\n",
    "print()\n",
    "print(df.query(\"a in @our_df.b1\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado por condiciones múltiples\n",
    "\n",
    "Es posible utilizar múltiples condiciones de manera tradicional. Para ello, podemos filtrar los DataFrames de pandas utilizando los signos  `&`, `|` y `~`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importar el df\n",
    "df = pd.read_csv('/datasets/vg_sales.csv')\n",
    "\n",
    "## Manipulación\n",
    "df['user_score'] = pd.to_numeric(df['user_score'], errors='coerce')\n",
    "\n",
    "## Filtrado\n",
    "df_filtered = df[(df[\"year_of_release\"] >= 1980) & (df[\"year_of_release\"]< 1990)]# escribe tu código aquí\n",
    "print(df_filtered.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible utilizar el método de ``query()` escribiendo las cadenas de consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtrado múltiple con querys\n",
    "developers = ['SquareSoft', 'Enix Corporation', 'Square Enix']\n",
    "cols = ['name', 'developer', 'na_sales', 'eu_sales', 'jp_sales']\n",
    "\n",
    "## Tres condiciones diferentes\n",
    "q_string = \"(na_sales > 0 and eu_sales > 0 and jp_sales >0) and (jp_sales > na_sales + eu_sales) and (developer in @developers)\"\n",
    "df_filtered = df.query(q_string)[cols]\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remplazo de valores con where()\n",
    "\n",
    "Es posible que cuando estemos realizando nuestros análisis exploratorios de datos (EDA), a menudo como parte del filtro de datos, implica modificación de los valores de las columnas. En esos casos podemos utilizar el mátodo `where()` para filtrar y modificaral mismo tiempo. \n",
    "\n",
    "El método `where()` en Pandas se puede entender como una manera de mantener ciertos valores en una columna que cumplen con una condición, y cambiar los valores que no la cumplen. En otras palabras, `where` se usa para cambiar **solo los valores que no cumplen** con la condición especificada, manteniendo los demás intactos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector de variables a cambiar \n",
    "genres = ['Puzzle', 'Strategy']\n",
    "\n",
    "## Cambio de \n",
    "df['genre'] = df['genre'].where(~df['genre'].isin(genres), 'Misc')\n",
    "\n",
    "print(df['genre'].value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, where está haciendo lo siguiente:\n",
    "\n",
    "- `~df['genre'].isin(genres)` es la condición. El símbolo ~ invierte la condición, es decir, selecciona todos los géneros que no están en la lista genres (en este caso, 'Puzzle' y 'Strategy').\n",
    "- Si el género no está en la lista `genres`, `where` deja el valor de `genre` tal cual está.\n",
    "- Si el género sí está en la lista `genres`, `where` lo reemplaza por `'Misc'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones especiales** \n",
    "\n",
    "1. **Para rellenar valores ausentes**, se puede usar el método where, pero una forma más sencilla es utilizar fillna().\n",
    "\n",
    "2. **Para reemplazar una palabra específica** en una columna o vector, también se puede hacer con where, pero el método replace() es más directo y fácil de usar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajar con tipos de datos\n",
    "--\n",
    "\n",
    "### Datos strings y numéricos\n",
    "Cuando pandas lee datos de un archivo de texto, automáticamente convierte los datos sin procesar al tipo de datos pandas.\n",
    "\n",
    "Por lo general, la conversión es directa: una columna que contenga solo números se leerá en automático como un tipo de datos `float` o `int`. Si una columna solo contiene palabras, se lee como un tipo de datos `object`. \n",
    "\n",
    "A veces, pandas no puede inferir el tipo de datos correcto o podría no ser lo que queremos. Cuando esto pasa, necesitamos intervenir y convertir por nuestra cuenta los valores al tipo correcto. Para poder convertir a un **tipo de dato específico** utilizamos el método `astype()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col1  col2\n",
      "0   1.0     3\n",
      "1   2.0     4\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   col1    2 non-null      float64\n",
      " 1   col2    2 non-null      int64  \n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 164.0 bytes\n"
     ]
    }
   ],
   "source": [
    "# Creación del dataframe a utilizar\n",
    "d = {'col1': [1.0, 2.0], 'col2': [3, 4]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "print()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  col1 col2\n",
      "0  1.0    3\n",
      "1  2.0    4\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   col1    2 non-null      object\n",
      " 1   col2    2 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 164.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Conversión de flotantes a strings u objects\n",
    "df_as_string = df.astype(\"str\")\n",
    "print(df_as_string)\n",
    "print()\n",
    "df_as_string.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La impresión del DataFrame se ve igual, pero si observamos la información de la df nos damos cuenta que se realizó la conversión numérica a tipo string u object. También podemos usar el método `astype()` en columnas individuales: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col1  col2\n",
      "0     1     3\n",
      "1     2     4\n",
      "\n",
      "col1    int64\n",
      "col2    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Conversión de float a entero \n",
    "df[\"col1\"] = df[\"col1\"]. astype(\"int\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario ser precavido al convertir entre tipos de datos, una buena práctica es evaluar si la operación de conversión podría llevar a cambios significativos en tus datos. Esto puede realizarse mediante la librería `numpy`utilizando la función `arrray_equal()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Importación de numpy\n",
    "import numpy as np\n",
    "\n",
    "## Creación de la df\n",
    "d = {'col1': [1.0, 2.0, 3.0, 4.0], 'col2': [5.0, 6.01, 7.0, 8.0]}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "## Comprobar si es seguro convertir 'col2'\n",
    "np.array_equal(df['col2'], df['col2'].astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings a valores numéricos\n",
    "A veces queremos conservar los valores del string que parecen números como tipos de datos string, algunos ejemplos incluyen IDs, códigos postales. Para ello es posible hacer la manipulación utilizando nuevamente `astype()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1    object\n",
      "col2     int64\n",
      "dtype: object\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '1.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m() \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m## Convertir string a entero\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcol1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '1.0'"
     ]
    }
   ],
   "source": [
    "d = {'col1': ['1.0', '2.0'], 'col2': ['3', '4']}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "# Convertir string a entero\n",
    "df['col2'] = df['col2'].astype('int')\n",
    "print(df.dtypes)\n",
    "print() \n",
    "\n",
    "## Convertir string a entero\n",
    "df['col1'] = df['col1'].astype('int') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, en algunos caos no se puede convertir los strings en números de esta forma. El problema es que queremos convertir un string a números enteros cuando los valores presentan decimales. Para forzar el cambio es necesario utilizar el método `to numeric`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1    float64\n",
      "col2      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Conversión forzada a numérico\n",
    "df['col1'] = pd.to_numeric(df['col1'])\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma predeterminada, `to_numeric()` no puede convertir strings con caracteres no numéricos o decimales en números. En cambio, devuelve un error. Es posible utilizar el parámetro `errors=` que sirve para determinar la acción implementada si encuentra un valor no válido: \n",
    "\n",
    "- `errors='raise'`: argumento predeterminado por el cual los valores inválidos generan errores, bloqueando la conversión a números para toda la columna.\n",
    "- `errors='coerce'`: los valores inválidos se reemplazan por NaN.\n",
    "- `errors='ignore'`: los valores inválidos simplemente se ignoran y se dejan sin cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajar con fechas y horas\n",
    "Otro tipo de datos que se puede manipular son los de fecha y hora. Esto se debe a que en el mundo se usan muchos formatos diferentes de fecha. En pandas es posible convertir strings a formatos de fecha utilizando el método `to_datetime()`, especificando el parámetro `format=`. Los códigos de formato señalados por el símbolo `%` se utilizan para especificar el formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "2010-12-17 12:38:00\n"
     ]
    }
   ],
   "source": [
    "## Objeto en tipo string\n",
    "string_date = '2010-12-17T12:38:00Z'\n",
    "\n",
    "## Conversión a tipo de fecha especificando el formato que tiene\n",
    "datetime_date = pd.to_datetime(string_date, format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "## Comprobación del cambio\n",
    "print(type(string_date))\n",
    "print(type(datetime_date))\n",
    "print(datetime_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen muchos símbolos de formato, pero solamente unos cuantos los usarás con regularidad. Por ejemplo:\n",
    "\n",
    "- `%d`: día del mes (01 a 31).\n",
    "- `%m`: mes (01 a 12).\n",
    "- `%Y`: año en cuatro dígitos (2019).\n",
    "- `%y`: año en dos dígitos (19).\n",
    "- `%H`: hora en formato de 24 horas.\n",
    "- `%I`: hora en formato de 12 horas.\n",
    "- `%M`: minutos (00 a 59).\n",
    "- `%S`: segundos (00 a 59)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando trabajamos con formatos de tipo `datetime`, en el fondo Python y pandas los siguen categorizando como estructuras de tipo series, por lo que al momento de querer acceder a sus valores individuales genera error. \n",
    "\n",
    "Para obtener atributos en las columnas de datos tipo `datetime`, se usa el objeto accesor `.dt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conversión de string a formato datetime\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "## Obtener los valores específicos de dia dentro del formato datetime\n",
    "df_days = df['InvoiceDate'].dt.day\n",
    "print(df_days.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajar con husos horarios\n",
    "Existen escenarios que tienen que ver con las zonas horarias al trabajar con datos `datetime`. Ejemplos de estos escenarios incluyen: \n",
    "\n",
    "- Datos provenientes de distintas zonas geográficas dependiendo de la ubicación rdonde se registran estos datos usando su hora local.\n",
    "- Trabajar con valores datetime de tu zona horaria, pero los interesados se encuentran en otra. \n",
    "\n",
    "En cualquier caso, debes saber cómo convertir entre distintas zonas horarias sin confundirte. Es ahí donde nos son útiles `.dt.tz_localize()` y `.dt.tz_convert()`.\n",
    "\n",
    "- **`.dt.tz_localize()`**. Permite asignar una zona horaria a una columna datetime para que tus datos \"tengan conocimiento\" de su zona horaria. \n",
    "- **`.dt.tz_convert()`**. Permite convertir una columna \"con conocimiento de su zona horaria\" en una zona horaria distinta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejemplo del uso `dt.tz_localize` para asignar el huso horario UTC a la columna \n",
    "df['InvoiceDate'] = df['InvoiceDate'].dt.tz_localize('UTC')\n",
    "\n",
    "## Ejemplo uso de `dt.tz_convert` pensando en que los valores ahora se encuentran en huso horario UTC\n",
    "df['InvoiceDate_NYC'] = df['InvoiceDate'].dt.tz_convert('America/New_York')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingeniería de características\n",
    "---\n",
    "Al proceso de crear nuevas columnas mediante las columnas originales en el conjunto de datos se le llama **ingeniería de características** y es de mucha utilidad en el machine learning. \n",
    "\n",
    "### Transformaciones utilizando operadres aritméticos\n",
    "Es posible crear nuevas columnas a partir de operadores aritméticos como la suma, porque la mayoría de las funciones matemáticas trabajan de forma vectorial: se aplican a columnas enteras a la vez, en lugar de recorrer cada valor de una columna. Esto proporciona un código más eficiente y conciso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la columna total_ventas y rellenarla\n",
    "df['total_sales'] = df['na_sales'] + df['eu_sales'] + df['jp_sales']\n",
    "\n",
    "# Crear la columna eu_sales_share y rellenarla\n",
    "df['eu_sales_share'] = df['eu_sales'] / df['total_sales']\n",
    "print(df['eu_sales_share'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columnas booleanas\n",
    "También es posible crear columnas que comprueben cierta condición. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'publisher'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'publisher'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# rear la columna is_nintendo y rellenarla\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_nintendo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublisher\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNintendo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_nintendo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'publisher'"
     ]
    }
   ],
   "source": [
    "# Crear la columna is_nintendo y rellenarla\n",
    "df['is_nintendo'] = df['publisher'] == 'Nintendo'\n",
    "print(df['is_nintendo'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columnas categóricas\n",
    "Si la columna de string representa un conjunto de categorías, es mucho mejor tratar esos valores directamente como **categorías**. La ventaja de trabajar con datos categóricos en vez de strings es que podemos ahorrar memoria y agilizar el análisis, sobre todo en el caso de grandes conjuntos de datos. Y esto se puede hacer con el tipo de datos `categorical`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear columnas con apply()\n",
    "Es muy común que en los análisis de datos necesitemos categorizar o segmentar a nuestras observaciones. es decir agrupar los datos en nuevas categorías que creamos. \n",
    "\n",
    "Para crear una columna de categoría es útil utilizar crear una función con bucles `elif` y posteriormente utilizar lel método `apply`, el cual toma valores de una columna DataFrame y les aplica una función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creación de la función `era_group`\n",
    "def era_group(year):\n",
    "\n",
    "    ## Descripción de la función\n",
    "    \"\"\"\n",
    "    La función devuelve el grupo de época de los juegos de acuerdo con el año de lanzamiento usando estas reglas:\n",
    "    —'retro'   para año < 2000\n",
    "    —'modern'  para 2000 <= año < 2010\n",
    "    —'recent'  para año >= 2010\n",
    "    —'unknown' para buscar valores año (NaN)\n",
    "    \"\"\"\n",
    "\n",
    "## Bucle de la función\n",
    "    if year < 2000:\n",
    "        return 'retro'\n",
    "    elif year < 2010:\n",
    "        return 'modern'\n",
    "    elif year >= 2010:\n",
    "        return 'recent'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "## Aplicación de apply para que a cada valor de la columna `year_of_release` lo etiquueta\n",
    "df['era_group'] = df['year_of_release'].apply(era_group)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método apply trabaja muy bien con una sola columna, pero si necesitamos que o haga en filas es necesario agregar el parámetro `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creación de la función \n",
    "\n",
    "def era_sales_group(row):\n",
    "\n",
    "## Descripción de la función \n",
    "    \"\"\"\n",
    "    La función devuelve una categoría de juegos según el año de lanzamiento y las ventas totales mediante las siguientes reglas:\n",
    "    —'retro'   para año < 2000 y las ventas totales < $1 million\n",
    "    —'modern'  para 2000 <= año < 2010 y las ventas totales < $1 million\n",
    "    —'recent'  para año >= 2010 y las ventas totales < $1 million\n",
    "    —'classic' para año < 2010 y las ventas totales >= $1 million\n",
    "    —'big hit' para año >= 2010 y las ventas totales >= $1 million\n",
    "    \"\"\"\n",
    "\n",
    "## Definición de las columnas a utilizar\n",
    "    year = row['year_of_release']\n",
    "    na_sales = row['na_sales']\n",
    "    eu_sales = row['eu_sales']\n",
    "    jp_sales = row['jp_sales']\n",
    "    \n",
    "    ## Operación que se utilizará después para el bucle\n",
    "    total_sales = na_sales + eu_sales + jp_sales\n",
    "    \n",
    "    ## Bucle\n",
    "    if year < 2000:\n",
    "        if total_sales < 1:\n",
    "            return 'retro'\n",
    "        else:\n",
    "            return 'classic'\n",
    "    if year < 2010:\n",
    "        if total_sales < 1:\n",
    "            return 'modern'\n",
    "        else:\n",
    "            return 'classic'\n",
    "    if year >= 2010:\n",
    "        if total_sales < 1:\n",
    "            return 'recent'\n",
    "        else:\n",
    "            return 'big hit'\n",
    "\n",
    "## Aplicar el método apply por columna\n",
    "df['game_category'] = df.apply(era_sales_group, axis=1)\n",
    "print(df.sample(5, random_state=321))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de datos\n",
    "---\n",
    "Cuando se crean objetos con `groupby` el resultado es un series. Si es una tabla cruzada, estos será con distintos índices para cada columna. Este objeto se denomina \"objeto en espera\" y se mostrará en forma de texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = df.groupby(['platform', 'genre'])\n",
    "print(grp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es por ello que este tipo de objetos `DataFrameGroupby`, necesitan formar parte de un *framework* de procesamiento de datos llamado **dividir-aplicar-combinar**, que consta de lo siguiente: \n",
    "\n",
    "1. **Dividir** los datos en grupos.\n",
    "2. **Aplicar** una función de agregación estadística a cada grupo.\n",
    "3. **Combinar** los resultados para cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En tres paso sería \n",
    "## Dividir el conjunto en grupos\n",
    "grp = df.groupby(['platform', 'genre'])\n",
    "\n",
    "## Aplicamos el método mean y  combinamos el resultado en un objeto Series grp['critic_score'].mean()\n",
    "mean_scores = grp['critic_score'].mean()\n",
    "print(mean_scores)\n",
    "\n",
    "## En un solo paso \n",
    "print(df.groupby(['platform', 'genre'])['critic_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de datos agrupados con agg() \n",
    "\n",
    "En estos casos solo se ha podido realizar una sola fufunción, pero a veces es necesario conocer más de una función de agregación. Para poder obtener más de una función es necesario utilizar el método `agg()`\n",
    "\n",
    "El método `agg()` usa un diccionario como entrada donde las claves son los nombres de columnas y los valores correspondientes son las funciones de agregación que quieres aplicarles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaración de los agregados que queremos conocer en diccionario\n",
    "agg_dict = {'critic_score': 'mean', 'jp_sales': 'sum'}\n",
    "\n",
    "## Agrupación de las variables\n",
    "grp = df.groupby(['platform', 'genre'])\n",
    "\n",
    "## Agregación\n",
    "print(grp.agg(agg_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluso puedes aplicar tus propias funciones personalizadas con `agg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definición de la función \n",
    "def double_it(sales):\n",
    "    sales = sales.sum() * 2 # multiplica la suma anterior por 2\n",
    "    return sales\n",
    "\n",
    "## Creación del diccionario\n",
    "agg_dict = {'jp_sales': double_it}\n",
    "\n",
    "## Agrupación \n",
    "grp = df.groupby(['platform', 'genre'])\n",
    "\n",
    "## Agregación\n",
    "print(grp.agg(agg_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tablas dinámicas \n",
    "\n",
    "Las **tablas dinámicas** son una gran herramienta para sintetizar conjuntos de datos y explorar sus diferentes dimensiones.\n",
    "\n",
    "El método para crear este tipo de tablas es mediante el método `pivot_table`. Los parámetros que utilizamos fueron:\n",
    "\n",
    "- `index=`: la columna cuyos valores se convierten en índices en la tabla dinámica;\n",
    "- `columns=`: la columna cuyos valores se convierten en columnas en la tabla dinámica;\n",
    "- `values=`: la columna cuyos valores queremos agregar en la tabla dinámica;\n",
    "- `aggfunc=`: la función de agregación que queremos aplicar a los valores en cada grupo de filas y columnas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generación de la pivot table\n",
    "pivot_data = df.pivot_table(index='genre',\n",
    "                            columns='platform',\n",
    "                            values='eu_sales',\n",
    "                            aggfunc='sum'\n",
    "                           )\n",
    "print(pivot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso de una tabla dinámica aquí es conveniente porque nos permite fácilmente excluir todas las columnas de df que no nos interesan para nuestro análisis. Además, puede ser más fácil de leer que el resultado equivalente de `groupby()`. \n",
    "\n",
    "El resultado de `groupby()` devuelve un objeto Series, mientras que `pivot_table()` devuelve un DataFrame. Ya sea que elijas usar `groupby()` o `pivot_table()` depende de lo que sea más conveniente para la tarea en cuestión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinar Dataframes con concat()\n",
    "\n",
    "En algunas ocasiones tendremos bases de datos o dataframes **relacionales**, esto significa que tendrán columnas relacionadas.\n",
    "\n",
    "En la práctica por ejemplo, si deseamos utilizar el método `groupby` para obtener la suma y aparte el promedio de la misma variable, un `concat()` puede sr útil para unir ambas columnas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtenemos el promedio de la valoración del público por distribuidor\n",
    "mean_score = df.groupby('publisher')['critic_score'].mean()\n",
    "\n",
    "## Obtenemos la suma del total de ventas por distribuidor\n",
    "df['total_sales'] = df['na_sales'] + df['eu_sales'] + df['jp_sales']\n",
    "num_sales = df.groupby('publisher')['total_sales'].sum()\n",
    "\n",
    "## Cmbinamos los dataframes de ambos mediante la misma columna que es el distribuidor\n",
    "df_concat = pd.concat([mean_score, num_sales], axis='columns')\n",
    "\n",
    "## Renombrar columnas\n",
    "df_concat.columns = ['avg_critic_score', 'total_sales']\n",
    "print(df_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, `concat()` espera una lista de objetos de tipo Series y/o DataFrame. Para obtener nuestro resultado, pasamos una lista de variables de Series a `concat()` y configuramos `axis='columns'` para asegurarnos de que se combinaran como columnas. Podemos utilizarlo para concatenar DataFrames:\n",
    "\n",
    "- Por filas, suponiendo que tengan el **mismo número de columnas**;\n",
    "- Por columnas si tienen el **mismo número de filas**.\n",
    "\n",
    "Para concatenar filas de DataFrames separados, podemos utilizar números enteros para el argumento `index=`, donde `index=0` concatenará filas e i`ndex=1` concatenará columnas.\n",
    "\n",
    "Debido a que los nombres de las columnas se conservan, una buena práctica es volver más descriptivos los nombres, para ello es util el uso del método `columns= []` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       author                         title\n",
      "0      Alcott                  Little Women\n",
      "1  Fitzgerald              The Great Gatsby\n",
      "2   Steinbeck               Of Mice and Men\n",
      "3       Twain  The Adventures of Tom Sawyer\n",
      "4   Hemingway       The Old Man and the Sea\n",
      "\n",
      "      author                               title\n",
      "0  Steinbeck                        East of Eden\n",
      "1      Twain  The Adventures of Huckleberry Finn\n",
      "2  Hemingway             For Whom the Bell Tolls\n",
      "3   Salinger              The Catcher in the Rye\n",
      "4  Hawthorne                 The Scarlett Letter\n"
     ]
    }
   ],
   "source": [
    "## Creación de las bases de datos\n",
    "\n",
    "first_pupil_df = pd.DataFrame(\n",
    "    {\n",
    "        'author': ['Alcott', 'Fitzgerald', 'Steinbeck', 'Twain', 'Hemingway'],\n",
    "        'title': ['Little Women',\n",
    "                  'The Great Gatsby',\n",
    "                  'Of Mice and Men',\n",
    "                  'The Adventures of Tom Sawyer',\n",
    "                  'The Old Man and the Sea'\n",
    "                 ],\n",
    "    }\n",
    ")\n",
    "second_pupil_df = pd.DataFrame(\n",
    "    {\n",
    "        'author': ['Steinbeck', 'Twain', 'Hemingway', 'Salinger', 'Hawthorne'],\n",
    "        'title': ['East of Eden',\n",
    "                  'The Adventures of Huckleberry Finn',\n",
    "                  'For Whom the Bell Tolls',\n",
    "                  'The Catcher in the Rye',\n",
    "                  'The Scarlett Letter'\n",
    "                 ],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "print(first_pupil_df)\n",
    "print()\n",
    "print(second_pupil_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `merge()` permite realizar diferentes tipos de combinaciones dependiendo de cómo se desee alinear los datos: \n",
    "\n",
    "1. **Inner Join**. Combina solo las filas que tienen **claves coincidentes** en ambos DataFrames. Útil cuando solo deseas los registros que tienen datos correspondientes en ambos DataFrames. Uso: `pd.merge(df1, df2, on='columna_común')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      author                       title_x                             title_y\n",
      "0  Steinbeck               Of Mice and Men                        East of Eden\n",
      "1      Twain  The Adventures of Tom Sawyer  The Adventures of Huckleberry Finn\n",
      "2  Hemingway       The Old Man and the Sea             For Whom the Bell Tolls\n"
     ]
    }
   ],
   "source": [
    "## Unión interna o interesección\n",
    "both_pupils = first_pupil_df.merge(second_pupil_df, on='author')\n",
    "print(both_pupils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Left Join**. Devuelve todas las filas del **DataFrame izquierdo** (`df1`) y las filas coincidentes del DataFrame derecho (`df2`). Si no hay coincidencia, los valores en el DataFrame derecho serán `NaN`. Útil cuando deseas mantener todos los registros del DataFrame izquierdo y agregar información del DataFrame derecho si está disponible. Uso: `pd.merge(df1, df2, on='columna_común', how='left')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       author                       title_x  \\\n",
      "0      Alcott                  Little Women   \n",
      "1  Fitzgerald              The Great Gatsby   \n",
      "2   Steinbeck               Of Mice and Men   \n",
      "3       Twain  The Adventures of Tom Sawyer   \n",
      "4   Hemingway       The Old Man and the Sea   \n",
      "\n",
      "                              title_y  \n",
      "0                                 NaN  \n",
      "1                                 NaN  \n",
      "2                        East of Eden  \n",
      "3  The Adventures of Huckleberry Finn  \n",
      "4             For Whom the Bell Tolls  \n"
     ]
    }
   ],
   "source": [
    "## Unión izquierda\n",
    "both_pupils = first_pupil_df.merge(second_pupil_df, on='author', how='left')\n",
    "print(both_pupils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Right Join**. Devuelve todas las filas del **DataFrame derecho** (`df2`) y las filas coincidentes del DataFrame izquierdo (`df1`). Si no hay coincidencia, los valores en el DataFrame izquierdo serán `NaN`. Útil cuando deseas mantener todos los registros del DataFrame derecho y agregar información del DataFrame izquierdo si está disponible. Uso: `pd.merge(df1, df2, on='columna_común', how='right')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      author                       title_x                             title_y\n",
      "0  Steinbeck               Of Mice and Men                        East of Eden\n",
      "1      Twain  The Adventures of Tom Sawyer  The Adventures of Huckleberry Finn\n",
      "2  Hemingway       The Old Man and the Sea             For Whom the Bell Tolls\n",
      "3   Salinger                           NaN              The Catcher in the Rye\n",
      "4  Hawthorne                           NaN                 The Scarlett Letter\n"
     ]
    }
   ],
   "source": [
    "## Unión derecha\n",
    "both_pupils = first_pupil_df.merge(second_pupil_df, on='author', how='right')\n",
    "print(both_pupils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Outer Join**. Devuelve **todas las filas** cuando hay una coincidencia en cualquiera de los DataFrames. Las filas sin coincidencia en uno de los DataFrames tendrán valores `NaN`. Útil cuando deseas obtener todas las filas de ambos DataFrames, sin importar si tienen coincidencias o no. Uso: `pd.merge(df1, df2, on='columna_común', how='outer')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       author                       title_x  \\\n",
      "0      Alcott                  Little Women   \n",
      "1  Fitzgerald              The Great Gatsby   \n",
      "2   Hawthorne                           NaN   \n",
      "3   Hemingway       The Old Man and the Sea   \n",
      "4    Salinger                           NaN   \n",
      "5   Steinbeck               Of Mice and Men   \n",
      "6       Twain  The Adventures of Tom Sawyer   \n",
      "\n",
      "                              title_y  \n",
      "0                                 NaN  \n",
      "1                                 NaN  \n",
      "2                 The Scarlett Letter  \n",
      "3             For Whom the Bell Tolls  \n",
      "4              The Catcher in the Rye  \n",
      "5                        East of Eden  \n",
      "6  The Adventures of Huckleberry Finn  \n"
     ]
    }
   ],
   "source": [
    "## Unión externa\n",
    "both_pupils = first_pupil_df.merge(second_pupil_df, on='author', how='outer')\n",
    "print(both_pupils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos notar, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
